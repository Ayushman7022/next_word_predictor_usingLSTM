This project is a Next-Word Prediction Model built using LSTM networks. The model is trained on a text corpus and predicts the most probable next word in a sentence, making it useful for applications like smart keyboards, chatbots, and text auto-completion.

üìö Features
Deep Learning Model: Uses LSTM layers for sequence learning.
Word Tokenization: Converts text into numerical sequences using Tokenizer.
Dropout for Regularization: Helps prevent overfitting during training.
Categorical Cross-Entropy Loss: Optimized for multi-class word prediction.
Validation Accuracy Tracking: Evaluates model performance during training.
üìù Dataset
The model is trained on a preprocessed text corpus where each sentence is tokenized and converted into sequential data. You can replace the dataset with your own text file.
